{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv, NNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Preprocessing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in connectomes\n",
    "test_connectome = pd.read_csv('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/widsdatathon2025/TEST/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')\n",
    "train_connectome = pd.read_csv('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/widsdatathon2025/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in survey data\n",
    "train_cat_quant = pd.read_csv('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Preprocessing/train_cat_quant_imputed.csv')\n",
    "test_cat_quant = pd.read_csv('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Preprocessing/test_cat_quant_imputed.csv')\n",
    "\n",
    "# Read in solutions \n",
    "solutions = pd.read_excel('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/widsdatathon2025/TRAIN_NEW/TRAINING_SOLUTIONS.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UmrK0vMLopoR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPaeQkhcjg7d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nb4EetVPm3gs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p4vPhVu91o4b</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M09PXs7arQ5E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id  ADHD_Outcome  Sex_F\n",
       "0   UmrK0vMLopoR             1      1\n",
       "1   CPaeQkhcjg7d             1      0\n",
       "2   Nb4EetVPm3gs             1      0\n",
       "3   p4vPhVu91o4b             1      1\n",
       "4   M09PXs7arQ5E             1      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoder\n",
    "\n",
    "Training the DAE using the training data only\n",
    "\n",
    "Construct a graph for each patient\n",
    "\n",
    "Layer options\n",
    "\n",
    "- Graph convolution network GCN layers\n",
    "\n",
    "- Graph attention network\n",
    "\n",
    "- GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_construct(df, num_regions=200):\n",
    "    graph_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        participant_id = row['participant_id']\n",
    "        participant_row = row.values[1:]  # Skip participant ID if present\n",
    "        adj_matrix = np.zeros((num_regions, num_regions))\n",
    "\n",
    "        # Fill adjacency matrix (upper triangle only)\n",
    "        idx = 0\n",
    "        for i in range(num_regions):\n",
    "            for j in range(i + 1, num_regions):\n",
    "                adj_matrix[i, j] = participant_row[idx]\n",
    "                adj_matrix[j, i] = participant_row[idx]\n",
    "                idx += 1\n",
    "\n",
    "        # Extract edge index and weights\n",
    "        i_idx, j_idx = np.triu_indices(num_regions, k=1)\n",
    "        edges = np.stack([i_idx, j_idx], axis=1)\n",
    "        edge_weights = adj_matrix[i_idx, j_idx]\n",
    "\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "        x = torch.eye(num_regions, dtype=torch.float)\n",
    "\n",
    "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_regions)\n",
    "        graph_data.participant_id = participant_id \n",
    "        # Add to list\n",
    "        graph_list.append(graph_data)\n",
    "\n",
    "    return graph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement \n",
    "train_graph_list = create_graph_construct(train_connectome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "sample_graph = train_graph_list[0]\n",
    "print(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A and B is the same as B and A\n",
    "def is_undirected(graph):\n",
    "    edge_index = graph.edge_index.numpy().T  # Shape: (num_edges, 2)\n",
    "    edge_set = set(map(tuple, edge_index))\n",
    "\n",
    "    for u, v in edge_set:\n",
    "        if (v, u) not in edge_set:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check one graph\n",
    "print(is_undirected(train_graph_list[0]))\n",
    "\n",
    "# check all graphs\n",
    "all_undirected = all(is_undirected(g) for g in train_graph_list)\n",
    "print(all_undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NetworkX (no weights)\n",
    "nx_graph = to_networkx(sample_graph, to_undirected=True)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw(nx_graph, with_labels=True, node_size=800, font_size=12)\n",
    "plt.title(\"Sample Graph Connectivity\")\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edge weights manually from edge_attr\n",
    "edge_weights = sample_graph.edge_attr.numpy()\n",
    "edges = sample_graph.edge_index.numpy().T\n",
    "\n",
    "# Convert to NetworkX (with weights)\n",
    "G = nx.Graph()\n",
    "for (i, j), w in zip(edges, edge_weights):\n",
    "    G.add_edge(i, j, weight=w)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Draw with edge labels (weights)\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, node_size=300, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "plt.title(\"Graph with Edge Weights\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges = []\n",
    "mean_weights = []\n",
    "max_weights = []\n",
    "min_weights = []\n",
    "\n",
    "for graph in train_graph_list:\n",
    "    num_edges.append(graph.edge_index.shape[1])\n",
    "    mean_weights.append(graph.edge_attr.mean().item())\n",
    "    max_weights.append(graph.edge_attr.max().item())\n",
    "    min_weights.append(graph.edge_attr.min().item())\n",
    "\n",
    "print(\"Avg number of edges:\", np.mean(num_edges))\n",
    "print(\"Mean weight:\", np.mean(mean_weights))\n",
    "print(\"Max weight:\", np.max(max_weights))\n",
    "print(\"Min weight:\", np.min(min_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAE construct\n",
    "\n",
    "Architecture\n",
    "- Encoder\n",
    "- Decoder\n",
    "- Noise mechanism \n",
    "- Loss Function\n",
    "\n",
    "Components\n",
    "- GCN layer\n",
    "- Input Feature - x = torch.eye(200) \n",
    "- Output target - Node features (x)\n",
    "    - DAE reconstructs enhanced node features\n",
    "    - GNN compatible\n",
    "    - captures both local structure and noise-robust patterns\n",
    "    - compact\n",
    "\n",
    "Noise mechnaisms:\n",
    "- masking edge weights\n",
    "- adding Gaussian noise to edge weights\n",
    "- Perturb X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph GCN Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout=0.2):\n",
    "\n",
    "        super(GraphDAE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Encoder layer: learns node embeddings, compressed node into latent_dim\n",
    "        self.conv1 = GCNConv(input_dim,hidden_dim)\n",
    "        self.conv_intermediate = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
    "\n",
    "\n",
    "        # Decoder layer: Fully connected layers to predict edge weights between node pairs\n",
    "        self.fullc1 = nn.Linear(latent_dim * 2, hidden_dim) \n",
    "        self.fullc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU() # non-linearity\n",
    "        self.sigmoid = nn.Sigmoid() # weights are [0,1]\n",
    "\n",
    "    def add_noise(self, edge_index, edge_attr, noise_factor=0.05):\n",
    "        \n",
    "        mask = torch.rand(edge_attr.size()) > self.dropout # Randomly drop edge weights\n",
    "        noisy_edge_attr = edge_attr.clone() # Clone to not modify original\n",
    "\n",
    "        # Using Gaussian noise to non-dropped edges * by noise factor\n",
    "        noisy_edge_attr[mask] += torch.randn(sum(mask)).to(edge_attr.device) * noise_factor # randomly generates a value and saves it to same device original tensor is stored\n",
    "        return edge_index, noisy_edge_attr\n",
    "    \n",
    "    # Encoder\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv_intermediate(x, edge_index)) # Encode the graph to latent node embeddings using GCN layers\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    # Decoder\n",
    "    def decode(self, z, edge_index):\n",
    "\n",
    "        # concatenate embeddings z[i] and z[j] represents each node\n",
    "        edge_features = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=-1)\n",
    "\n",
    "        x = self.relu(self.fullc1(edge_features))\n",
    "\n",
    "        x = self.fullc2(x).squeeze()\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    # Combines all steps: noise, encode, and decode\n",
    "    # Returns reconstructed edge weights nad z a tensor object of latent node embeddings\n",
    "    def forward(self, data):\n",
    "\n",
    "        x,edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        # Normalize to [0,1]\n",
    "        edge_attr = (edge_attr - edge_attr.min()) / (edge_attr.max() - edge_attr.min() + 1e-6)\n",
    "        # Add noise \n",
    "        noisy_edge_index, noisy_edge_attr = self.add_noise(edge_index, edge_attr)\n",
    "        # Encode noisy graph to latent embeddings\n",
    "        z = self.encode(x, noisy_edge_index, noisy_edge_attr)\n",
    "        # Decode to reconstruct clean edge weights\n",
    "        recon_edge_attr = self.decode(z, edge_index)\n",
    "\n",
    "        return recon_edge_attr, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decoder\n",
    "\n",
    "# Parameters\n",
    "input_dim = 200 # number of nodes\n",
    "hidden_dim = 128 # number of neurons in nn\n",
    "latent_dim = 256 # latent embedding size per ned\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "model = GraphDAE(input_dim, hidden_dim, latent_dim).to(device) # move mode to GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss() # metric\n",
    "\n",
    "graph_list = create_graph_construct(train_connectome, num_regions=200)\n",
    "loader = DataLoader(graph_list, batch_size=batch_size, shuffle=True) # Organizes graph_list into batches, since our graph_list is a list of Data objects, training on one at a time is slow\n",
    "# and all at once is too much\n",
    "\n",
    "# Train loop for batches\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        recon_edge_attr, z = model(batch)\n",
    "\n",
    "        loss = criterion(recon_edge_attr, batch.edge_attr)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "avg_loss = total_loss / len(loader)\n",
    "\n",
    "print(f\"Epoch {epoch+1}, Loss: {total_loss / len(loader)}\")\n",
    "print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
    "    # Early stopping if loss plateaus\n",
    "if avg_loss < 0.005:  # Target threshold\n",
    "    print(\"Stopping early\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Models/graph_dae.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Graph Denoising Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonGraphDAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout=0.2):\n",
    "        \n",
    "        super(NonGraphDAE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Encoder layer: learns node embeddings, compressed node into latent_dim\n",
    "        self.enc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.enc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder layer: Fully connected layers to predict edge weights between node pairs\n",
    "        self.dec1 = nn.Linear(latent_dim, hidden_dim) \n",
    "        self.dec2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU() # non-linearity\n",
    "\n",
    "    def add_noise(self, x, noise_factor=0.05):\n",
    "        \n",
    "        noisy_x = x.clone() # Clone to not modify original\n",
    "        mask = torch.rand(x.size(), device = x.device) > self.dropout # Randomly drop edge weights\n",
    "        noise = torch.randn_like(x) * noise_factor\n",
    "        noisy_x[mask] += noise[mask]\n",
    "\n",
    "        return noisy_x\n",
    "    \n",
    "    # Encoder\n",
    "    def encode(self, x):\n",
    "\n",
    "        x = self.relu(self.enc1(x))# Encode the graph to latent node embeddings using GCN layers\n",
    "        x = self.enc2(x)\n",
    "        return x\n",
    "    \n",
    "    # Decoder\n",
    "    def decode(self, z):\n",
    "\n",
    "        x = self.relu(self.dec1(z))\n",
    "        x = self.dec2(x)\n",
    "        return x\n",
    "    \n",
    "    # Combines all steps: noise, encode, and decode\n",
    "    # Returns reconstructed edge weights nad z a tensor object of latent node embeddings\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Add noise \n",
    "        noisy_x = self.add_noise(x) if self.training else x\n",
    "        # Encode noisy graph to latent embeddings\n",
    "        z = self.encode(noisy_x)\n",
    "        # Decode to reconstruct clean edge weights\n",
    "        recon_x = self.decode(z)\n",
    "\n",
    "        return recon_x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1213, 19900])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "features = train_connectome.drop(columns=['participant_id']).values\n",
    "x = torch.tensor(features, dtype=torch.float32)\n",
    "print(\"x shape:\", x.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Avg Loss: 0.043409\n",
      "Epoch [2/150], Avg Loss: 0.034291\n",
      "Epoch [3/150], Avg Loss: 0.032688\n",
      "Epoch [4/150], Avg Loss: 0.031614\n",
      "Epoch [5/150], Avg Loss: 0.030623\n",
      "Epoch [6/150], Avg Loss: 0.029971\n",
      "Epoch [7/150], Avg Loss: 0.029570\n",
      "Epoch [8/150], Avg Loss: 0.029396\n",
      "Epoch [9/150], Avg Loss: 0.029220\n",
      "Epoch [10/150], Avg Loss: 0.028964\n",
      "Epoch [11/150], Avg Loss: 0.028776\n",
      "Epoch [12/150], Avg Loss: 0.028616\n",
      "Epoch [13/150], Avg Loss: 0.028466\n",
      "Epoch [14/150], Avg Loss: 0.028430\n",
      "Epoch [15/150], Avg Loss: 0.028368\n",
      "Epoch [16/150], Avg Loss: 0.028289\n",
      "Epoch [17/150], Avg Loss: 0.028191\n",
      "Epoch [18/150], Avg Loss: 0.028143\n",
      "Epoch [19/150], Avg Loss: 0.028089\n",
      "Epoch [20/150], Avg Loss: 0.028063\n",
      "Epoch [21/150], Avg Loss: 0.027941\n",
      "Epoch [22/150], Avg Loss: 0.027831\n",
      "Epoch [23/150], Avg Loss: 0.027978\n",
      "Epoch [24/150], Avg Loss: 0.027862\n",
      "Epoch [25/150], Avg Loss: 0.027783\n",
      "Epoch [26/150], Avg Loss: 0.027716\n",
      "Epoch [27/150], Avg Loss: 0.027610\n",
      "Epoch [28/150], Avg Loss: 0.027606\n",
      "Epoch [29/150], Avg Loss: 0.027602\n",
      "Epoch [30/150], Avg Loss: 0.027531\n",
      "Epoch [31/150], Avg Loss: 0.027585\n",
      "Epoch [32/150], Avg Loss: 0.027582\n",
      "Epoch [33/150], Avg Loss: 0.027563\n",
      "Epoch [34/150], Avg Loss: 0.027523\n",
      "Epoch [35/150], Avg Loss: 0.027411\n",
      "Epoch [36/150], Avg Loss: 0.027367\n",
      "Epoch [37/150], Avg Loss: 0.027364\n",
      "Epoch [38/150], Avg Loss: 0.027342\n",
      "Epoch [39/150], Avg Loss: 0.027358\n",
      "Epoch [40/150], Avg Loss: 0.027192\n",
      "Epoch [41/150], Avg Loss: 0.027170\n",
      "Epoch [42/150], Avg Loss: 0.027159\n",
      "Epoch [43/150], Avg Loss: 0.027195\n",
      "Epoch [44/150], Avg Loss: 0.027255\n",
      "Epoch [45/150], Avg Loss: 0.027253\n",
      "Epoch [46/150], Avg Loss: 0.027250\n",
      "Epoch [47/150], Avg Loss: 0.027221\n",
      "Epoch [48/150], Avg Loss: 0.027156\n",
      "Epoch [49/150], Avg Loss: 0.027078\n",
      "Epoch [50/150], Avg Loss: 0.027021\n",
      "Epoch [51/150], Avg Loss: 0.027041\n",
      "Epoch [52/150], Avg Loss: 0.027040\n",
      "Epoch [53/150], Avg Loss: 0.026998\n",
      "Epoch [54/150], Avg Loss: 0.026946\n",
      "Epoch [55/150], Avg Loss: 0.027003\n",
      "Epoch [56/150], Avg Loss: 0.026938\n",
      "Epoch [57/150], Avg Loss: 0.026957\n",
      "Epoch [58/150], Avg Loss: 0.026983\n",
      "Epoch [59/150], Avg Loss: 0.027039\n",
      "Epoch [60/150], Avg Loss: 0.026987\n",
      "Epoch [61/150], Avg Loss: 0.026956\n",
      "Epoch [62/150], Avg Loss: 0.026925\n",
      "Epoch [63/150], Avg Loss: 0.026879\n",
      "Epoch [64/150], Avg Loss: 0.026889\n",
      "Epoch [65/150], Avg Loss: 0.026904\n",
      "Epoch [66/150], Avg Loss: 0.026861\n",
      "Epoch [67/150], Avg Loss: 0.026895\n",
      "Epoch [68/150], Avg Loss: 0.026873\n",
      "Epoch [69/150], Avg Loss: 0.026873\n",
      "Epoch [70/150], Avg Loss: 0.026957\n",
      "Epoch [71/150], Avg Loss: 0.026946\n",
      "Epoch [72/150], Avg Loss: 0.026937\n",
      "Epoch [73/150], Avg Loss: 0.026885\n",
      "Epoch [74/150], Avg Loss: 0.026956\n",
      "Epoch [75/150], Avg Loss: 0.026986\n",
      "Epoch [76/150], Avg Loss: 0.026831\n",
      "Epoch [77/150], Avg Loss: 0.026711\n",
      "Epoch [78/150], Avg Loss: 0.026723\n",
      "Epoch [79/150], Avg Loss: 0.026758\n",
      "Epoch [80/150], Avg Loss: 0.026785\n",
      "Epoch [81/150], Avg Loss: 0.026775\n",
      "Epoch [82/150], Avg Loss: 0.026743\n",
      "Epoch [83/150], Avg Loss: 0.026799\n",
      "Epoch [84/150], Avg Loss: 0.026733\n",
      "Epoch [85/150], Avg Loss: 0.026787\n",
      "Epoch [86/150], Avg Loss: 0.026786\n",
      "Epoch [87/150], Avg Loss: 0.026914\n",
      "Epoch [88/150], Avg Loss: 0.026816\n",
      "Epoch [89/150], Avg Loss: 0.026789\n",
      "Epoch [90/150], Avg Loss: 0.026820\n",
      "Epoch [91/150], Avg Loss: 0.026792\n",
      "Epoch [92/150], Avg Loss: 0.026775\n",
      "Epoch [93/150], Avg Loss: 0.026714\n",
      "Epoch [94/150], Avg Loss: 0.026756\n",
      "Epoch [95/150], Avg Loss: 0.026739\n",
      "Epoch [96/150], Avg Loss: 0.026730\n",
      "Epoch [97/150], Avg Loss: 0.026812\n",
      "Epoch [98/150], Avg Loss: 0.026724\n",
      "Epoch [99/150], Avg Loss: 0.026708\n",
      "Epoch [100/150], Avg Loss: 0.026659\n",
      "Epoch [101/150], Avg Loss: 0.026622\n",
      "Epoch [102/150], Avg Loss: 0.026634\n",
      "Epoch [103/150], Avg Loss: 0.026570\n",
      "Epoch [104/150], Avg Loss: 0.026590\n",
      "Epoch [105/150], Avg Loss: 0.026667\n",
      "Epoch [106/150], Avg Loss: 0.026716\n",
      "Epoch [107/150], Avg Loss: 0.026657\n",
      "Epoch [108/150], Avg Loss: 0.026586\n",
      "Epoch [109/150], Avg Loss: 0.026612\n",
      "Epoch [110/150], Avg Loss: 0.026623\n",
      "Epoch [111/150], Avg Loss: 0.026692\n",
      "Epoch [112/150], Avg Loss: 0.026625\n",
      "Epoch [113/150], Avg Loss: 0.026649\n",
      "Epoch [114/150], Avg Loss: 0.026683\n",
      "Epoch [115/150], Avg Loss: 0.026603\n",
      "Epoch [116/150], Avg Loss: 0.026623\n",
      "Epoch [117/150], Avg Loss: 0.026817\n",
      "Epoch [118/150], Avg Loss: 0.026712\n",
      "Epoch [119/150], Avg Loss: 0.026716\n",
      "Epoch [120/150], Avg Loss: 0.026909\n",
      "Epoch [121/150], Avg Loss: 0.026894\n",
      "Epoch [122/150], Avg Loss: 0.026811\n",
      "Epoch [123/150], Avg Loss: 0.026764\n",
      "Epoch [124/150], Avg Loss: 0.026726\n",
      "Epoch [125/150], Avg Loss: 0.026663\n",
      "Epoch [126/150], Avg Loss: 0.026572\n",
      "Epoch [127/150], Avg Loss: 0.026546\n",
      "Epoch [128/150], Avg Loss: 0.026646\n",
      "Epoch [129/150], Avg Loss: 0.026682\n",
      "Epoch [130/150], Avg Loss: 0.026578\n",
      "Epoch [131/150], Avg Loss: 0.026529\n",
      "Epoch [132/150], Avg Loss: 0.026501\n",
      "Epoch [133/150], Avg Loss: 0.026490\n",
      "Epoch [134/150], Avg Loss: 0.026546\n",
      "Epoch [135/150], Avg Loss: 0.026620\n",
      "Epoch [136/150], Avg Loss: 0.026539\n",
      "Epoch [137/150], Avg Loss: 0.026529\n",
      "Epoch [138/150], Avg Loss: 0.026535\n",
      "Epoch [139/150], Avg Loss: 0.026556\n",
      "Epoch [140/150], Avg Loss: 0.026467\n",
      "Epoch [141/150], Avg Loss: 0.026516\n",
      "Epoch [142/150], Avg Loss: 0.026521\n",
      "Epoch [143/150], Avg Loss: 0.026484\n",
      "Epoch [144/150], Avg Loss: 0.026473\n",
      "Epoch [145/150], Avg Loss: 0.026554\n",
      "Epoch [146/150], Avg Loss: 0.026579\n",
      "Epoch [147/150], Avg Loss: 0.026555\n",
      "Epoch [148/150], Avg Loss: 0.026522\n",
      "Epoch [149/150], Avg Loss: 0.026607\n",
      "Epoch [150/150], Avg Loss: 0.026565\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_dim = x.shape[1]  # 19900\n",
    "hidden_dim = 256\n",
    "latent_dim = 512\n",
    "dropout = 0.3\n",
    "noise_factor = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model\n",
    "model = NonGraphDAE(input_dim, hidden_dim, latent_dim, dropout)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(x)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 150\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_x, in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, z = model(batch_x)\n",
    "        loss = criterion(recon_x, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Models/nongraph_dae_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids = train_connectome['participant_id']\n",
    "connectome_features = train_connectome.drop(columns=['participant_id']).values\n",
    "\n",
    "# Convert to tensor (no scaling needed)\n",
    "X = torch.tensor(connectome_features, dtype=torch.float32)\n",
    "\n",
    "# Load the full model\n",
    "autoencoder = torch.load('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Models/nongraph_dae_full.pth', \n",
    "                         map_location=torch.device('cpu'))\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = autoencoder.to(device)\n",
    "X = X.to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    _, z = autoencoder(X)\n",
    "    encoded_features = z.cpu().numpy()\n",
    "\n",
    "# Create DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=[f\"latent_{i}\" for i in range(encoded_features.shape[1])])\n",
    "encoded_df[\"participant_id\"] = ids.values\n",
    "encoded_df = encoded_df[['participant_id'] + [col for col in encoded_df.columns if col != 'participant_id']]\n",
    "\n",
    "# Merge solutions and survey data\n",
    "\n",
    "# Save DataFrame\n",
    "encoded_df.to_csv('/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Models/.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Graph Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonGraphAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout=0.2):\n",
    "        \n",
    "        super(NonGraphAE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Encoder layer: learns node embeddings, compressed node into latent_dim\n",
    "        self.enc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.enc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder layer: Fully connected layers to predict edge weights between node pairs\n",
    "        self.dec1 = nn.Linear(latent_dim, hidden_dim) \n",
    "        self.dec2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU() # non-linearity\n",
    "    \n",
    "    # Encoder\n",
    "    def encode(self, x):\n",
    "\n",
    "        x = self.relu(self.enc1(x))# Encode the graph to latent node embeddings using GCN layers\n",
    "        x = self.enc2(x)\n",
    "        return x\n",
    "    \n",
    "    # Decoder\n",
    "    def decode(self, z):\n",
    "\n",
    "        x = self.relu(self.dec1(z))\n",
    "        x = self.dec2(x)\n",
    "        return x\n",
    "    \n",
    "    # Combines all steps: noise, encode, and decode\n",
    "    # Returns reconstructed edge weights nad z a tensor object of latent node embeddings\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Encode noisy graph to latent embeddings\n",
    "        z = self.encode(x)\n",
    "        # Decode to reconstruct clean edge weights\n",
    "        recon_x = self.decode(z)\n",
    "\n",
    "        return recon_x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Avg Loss: 0.043744\n",
      "Epoch [2/150], Avg Loss: 0.034374\n",
      "Epoch [3/150], Avg Loss: 0.032716\n",
      "Epoch [4/150], Avg Loss: 0.031329\n",
      "Epoch [5/150], Avg Loss: 0.030440\n",
      "Epoch [6/150], Avg Loss: 0.029777\n",
      "Epoch [7/150], Avg Loss: 0.029247\n",
      "Epoch [8/150], Avg Loss: 0.028844\n",
      "Epoch [9/150], Avg Loss: 0.028536\n",
      "Epoch [10/150], Avg Loss: 0.028273\n",
      "Epoch [11/150], Avg Loss: 0.028062\n",
      "Epoch [12/150], Avg Loss: 0.027887\n",
      "Epoch [13/150], Avg Loss: 0.027792\n",
      "Epoch [14/150], Avg Loss: 0.027576\n",
      "Epoch [15/150], Avg Loss: 0.027402\n",
      "Epoch [16/150], Avg Loss: 0.027281\n",
      "Epoch [17/150], Avg Loss: 0.027115\n",
      "Epoch [18/150], Avg Loss: 0.026932\n",
      "Epoch [19/150], Avg Loss: 0.026856\n",
      "Epoch [20/150], Avg Loss: 0.026742\n",
      "Epoch [21/150], Avg Loss: 0.026616\n",
      "Epoch [22/150], Avg Loss: 0.026536\n",
      "Epoch [23/150], Avg Loss: 0.026711\n",
      "Epoch [24/150], Avg Loss: 0.026573\n",
      "Epoch [25/150], Avg Loss: 0.026358\n",
      "Epoch [26/150], Avg Loss: 0.026257\n",
      "Epoch [27/150], Avg Loss: 0.026231\n",
      "Epoch [28/150], Avg Loss: 0.026096\n",
      "Epoch [29/150], Avg Loss: 0.026111\n",
      "Epoch [30/150], Avg Loss: 0.026054\n",
      "Epoch [31/150], Avg Loss: 0.026046\n",
      "Epoch [32/150], Avg Loss: 0.026111\n",
      "Epoch [33/150], Avg Loss: 0.026029\n",
      "Epoch [34/150], Avg Loss: 0.025949\n",
      "Epoch [35/150], Avg Loss: 0.025797\n",
      "Epoch [36/150], Avg Loss: 0.025772\n",
      "Epoch [37/150], Avg Loss: 0.025720\n",
      "Epoch [38/150], Avg Loss: 0.025727\n",
      "Epoch [39/150], Avg Loss: 0.025777\n",
      "Epoch [40/150], Avg Loss: 0.025756\n",
      "Epoch [41/150], Avg Loss: 0.025681\n",
      "Epoch [42/150], Avg Loss: 0.025630\n",
      "Epoch [43/150], Avg Loss: 0.025612\n",
      "Epoch [44/150], Avg Loss: 0.025606\n",
      "Epoch [45/150], Avg Loss: 0.025528\n",
      "Epoch [46/150], Avg Loss: 0.025479\n",
      "Epoch [47/150], Avg Loss: 0.025482\n",
      "Epoch [48/150], Avg Loss: 0.025405\n",
      "Epoch [49/150], Avg Loss: 0.025360\n",
      "Epoch [50/150], Avg Loss: 0.025396\n",
      "Epoch [51/150], Avg Loss: 0.025473\n",
      "Epoch [52/150], Avg Loss: 0.025474\n",
      "Epoch [53/150], Avg Loss: 0.025465\n",
      "Epoch [54/150], Avg Loss: 0.025408\n",
      "Epoch [55/150], Avg Loss: 0.025452\n",
      "Epoch [56/150], Avg Loss: 0.025394\n",
      "Epoch [57/150], Avg Loss: 0.025303\n",
      "Epoch [58/150], Avg Loss: 0.025301\n",
      "Epoch [59/150], Avg Loss: 0.025304\n",
      "Epoch [60/150], Avg Loss: 0.025323\n",
      "Epoch [61/150], Avg Loss: 0.025231\n",
      "Epoch [62/150], Avg Loss: 0.025290\n",
      "Epoch [63/150], Avg Loss: 0.025222\n",
      "Epoch [64/150], Avg Loss: 0.025183\n",
      "Epoch [65/150], Avg Loss: 0.025217\n",
      "Epoch [66/150], Avg Loss: 0.025231\n",
      "Epoch [67/150], Avg Loss: 0.025243\n",
      "Epoch [68/150], Avg Loss: 0.025175\n",
      "Epoch [69/150], Avg Loss: 0.025166\n",
      "Epoch [70/150], Avg Loss: 0.025267\n",
      "Epoch [71/150], Avg Loss: 0.025174\n",
      "Epoch [72/150], Avg Loss: 0.025139\n",
      "Epoch [73/150], Avg Loss: 0.025198\n",
      "Epoch [74/150], Avg Loss: 0.025115\n",
      "Epoch [75/150], Avg Loss: 0.025130\n",
      "Epoch [76/150], Avg Loss: 0.025123\n",
      "Epoch [77/150], Avg Loss: 0.025091\n",
      "Epoch [78/150], Avg Loss: 0.025052\n",
      "Epoch [79/150], Avg Loss: 0.025044\n",
      "Epoch [80/150], Avg Loss: 0.025087\n",
      "Epoch [81/150], Avg Loss: 0.025081\n",
      "Epoch [82/150], Avg Loss: 0.025032\n",
      "Epoch [83/150], Avg Loss: 0.025039\n",
      "Epoch [84/150], Avg Loss: 0.025083\n",
      "Epoch [85/150], Avg Loss: 0.025062\n",
      "Epoch [86/150], Avg Loss: 0.025034\n",
      "Epoch [87/150], Avg Loss: 0.024983\n",
      "Epoch [88/150], Avg Loss: 0.025061\n",
      "Epoch [89/150], Avg Loss: 0.025117\n",
      "Epoch [90/150], Avg Loss: 0.025133\n",
      "Epoch [91/150], Avg Loss: 0.025133\n",
      "Epoch [92/150], Avg Loss: 0.024986\n",
      "Epoch [93/150], Avg Loss: 0.024910\n",
      "Epoch [94/150], Avg Loss: 0.024948\n",
      "Epoch [95/150], Avg Loss: 0.024950\n",
      "Epoch [96/150], Avg Loss: 0.025073\n",
      "Epoch [97/150], Avg Loss: 0.024953\n",
      "Epoch [98/150], Avg Loss: 0.024852\n",
      "Epoch [99/150], Avg Loss: 0.024894\n",
      "Epoch [100/150], Avg Loss: 0.024940\n",
      "Epoch [101/150], Avg Loss: 0.024966\n",
      "Epoch [102/150], Avg Loss: 0.024938\n",
      "Epoch [103/150], Avg Loss: 0.024988\n",
      "Epoch [104/150], Avg Loss: 0.024959\n",
      "Epoch [105/150], Avg Loss: 0.024945\n",
      "Epoch [106/150], Avg Loss: 0.024947\n",
      "Epoch [107/150], Avg Loss: 0.025060\n",
      "Epoch [108/150], Avg Loss: 0.025055\n",
      "Epoch [109/150], Avg Loss: 0.024891\n",
      "Epoch [110/150], Avg Loss: 0.024869\n",
      "Epoch [111/150], Avg Loss: 0.024838\n",
      "Epoch [112/150], Avg Loss: 0.024855\n",
      "Epoch [113/150], Avg Loss: 0.024856\n",
      "Epoch [114/150], Avg Loss: 0.024885\n",
      "Epoch [115/150], Avg Loss: 0.024792\n",
      "Epoch [116/150], Avg Loss: 0.024865\n",
      "Epoch [117/150], Avg Loss: 0.024797\n",
      "Epoch [118/150], Avg Loss: 0.024813\n",
      "Epoch [119/150], Avg Loss: 0.024854\n",
      "Epoch [120/150], Avg Loss: 0.024848\n",
      "Epoch [121/150], Avg Loss: 0.024846\n",
      "Epoch [122/150], Avg Loss: 0.024872\n",
      "Epoch [123/150], Avg Loss: 0.024842\n",
      "Epoch [124/150], Avg Loss: 0.024825\n",
      "Epoch [125/150], Avg Loss: 0.024869\n",
      "Epoch [126/150], Avg Loss: 0.024781\n",
      "Epoch [127/150], Avg Loss: 0.024765\n",
      "Epoch [128/150], Avg Loss: 0.024863\n",
      "Epoch [129/150], Avg Loss: 0.024868\n",
      "Epoch [130/150], Avg Loss: 0.024817\n",
      "Epoch [131/150], Avg Loss: 0.024737\n",
      "Epoch [132/150], Avg Loss: 0.024772\n",
      "Epoch [133/150], Avg Loss: 0.024706\n",
      "Epoch [134/150], Avg Loss: 0.024801\n",
      "Epoch [135/150], Avg Loss: 0.024756\n",
      "Epoch [136/150], Avg Loss: 0.024759\n",
      "Epoch [137/150], Avg Loss: 0.024784\n",
      "Epoch [138/150], Avg Loss: 0.024781\n",
      "Epoch [139/150], Avg Loss: 0.024757\n",
      "Epoch [140/150], Avg Loss: 0.024807\n",
      "Epoch [141/150], Avg Loss: 0.024793\n",
      "Epoch [142/150], Avg Loss: 0.024883\n",
      "Epoch [143/150], Avg Loss: 0.024822\n",
      "Epoch [144/150], Avg Loss: 0.024801\n",
      "Epoch [145/150], Avg Loss: 0.024773\n",
      "Epoch [146/150], Avg Loss: 0.024740\n",
      "Epoch [147/150], Avg Loss: 0.024701\n",
      "Epoch [148/150], Avg Loss: 0.024766\n",
      "Epoch [149/150], Avg Loss: 0.024715\n",
      "Epoch [150/150], Avg Loss: 0.024795\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_dim = x.shape[1]  # 19900\n",
    "hidden_dim = 256\n",
    "latent_dim = 512\n",
    "dropout = 0.3\n",
    "noise_factor = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model\n",
    "model = NonGraphAE(input_dim, hidden_dim, latent_dim, dropout)\n",
    "\n",
    "# Move to GPU \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(x)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 150\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_x, in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, z = model(batch_x) \n",
    "        loss = criterion(recon_x, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/Users/rubyc/Desktop/Datathon/WIDS_Datathon2025_Team/Archive/Models/nongraph_autoencoder.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
